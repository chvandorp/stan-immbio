{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.integrate import solve_ivp\n",
    "import cmdstanpy\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from stancourse import plots\n",
    "\n",
    "if os.name == \"nt\": ## adds compiler to path in Windows\n",
    "    cmdstanpy.utils.cxx_toolchain_path() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Contents\n",
    "\n",
    "* Goals of this Webinar\n",
    "* Jupyter notebook\n",
    "* Bayesian Inference\n",
    "* Markov-Chain Monte Carlo\n",
    "* Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals of this Webinar\n",
    "\n",
    "**Goal 1:** Get some understanding of how Stan works: May help with debugging\n",
    "\n",
    "**Goal 2:** Introduction to programming with Stan: Simple models, programming techniques\n",
    "\n",
    "Aspects of immunobiology / viral dynamics / epidemiology models and data\n",
    "* Non-linear, dynamical models\n",
    "* Repeated experiments (panel data)\n",
    "\n",
    "**Goal 3:** ODE models in Stan and multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jupyter notebooks\n",
    "\n",
    "* Selected cells either have a <span style=\"color:blue\">blue</span> or <span style=\"color:green\">green</span> border\n",
    "    * <span style=\"color:blue\">blue cells are in \"command mode\"</span>\n",
    "    * <span style=\"color:green\">green cells are in \"edit mode\"</span>\n",
    "* Execute a selected cell with `shift-enter` or `ctrl-enter`. `shift-enter` will select the next cell. \n",
    "* **WARNING** Make sure that the cell is in <span style=\"color:green\">edit mode</span> before typing in the cell\n",
    "* Switch to edit mode by pressing `enter` or clicking on the cell\n",
    "* The notebooks contain \"code\" cells and \"markdown\" cells\n",
    "* double click on a \"markdown\" cell to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## code cell: type python code here\n",
    "print(\"Welcome to this Stan Webinar\")\n",
    "sts.norm.rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Inference\n",
    "**Definitions and Bayes Theorem**\n",
    "\n",
    "Bayesian model \"ingredients\":\n",
    "\n",
    "1. <span style=\"color:blue\">Prior distribution $\\pi(\\theta)$</span> of parameters $\\theta \\in \\Theta$ encodes prior information about model **before we've seen our data**\n",
    "2. <span style=\"color:purple\">Likelihood function $L(D | \\theta)$</span> determines the likelihood of the data $D$, given parameters $\\theta$\n",
    "\n",
    "Bayes theorem gives the <span style=\"color:red\">posterior density</span> of the parameters $\\theta$, **given the data $D$**\n",
    "\n",
    "\\begin{equation}\n",
    "\\color{red}{P(\\theta | D)} = \\frac{\\color{blue}{\\pi(\\theta)} \\color{purple}{L(D | \\theta)}}{Z(D)}\n",
    "\\end{equation}\n",
    "\n",
    "* The marginal probability of the data $Z(D) = \\int_{\\Theta} \\pi(\\theta) L(D|\\theta) d\\theta$ is amost always intractable\n",
    "* Let $Q(\\theta | D) = \\color{blue}{\\pi(\\theta)} \\color{purple}{L(D | \\theta)}$ denote the un-normalized posterior density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Inference\n",
    "**Example**\n",
    "\n",
    "Take $N$ random samples, $K$ are \"positive\" ($N-K$ are \"negative\"). Goal: Estimate fraction $\\theta$ of positive individuals in the population\n",
    "* <span style=\"color:blue\">prior distribution $\\theta \\sim {\\rm Beta}(\\alpha, \\beta)$</span>\n",
    "* <span style=\"color:purple\">Binomial likelihood function $L(K | p) = \\binom{N}{K} \\theta^K (1-\\theta)^{N-K}$</span>\n",
    "* Unnormalized posterior\n",
    "\\begin{equation}\n",
    "Q(\\theta|K) = \\color{blue}{\\pi(\\theta)} \\color{purple}{L(\\theta,K)} = \\color{blue}{\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}}  \\color{purple}{\\binom{N}{K} \\theta^K (1-\\theta)^{N-K}}\n",
    "\\end{equation}\n",
    "* Normalizing constant\n",
    "\\begin{equation}\n",
    "Z(K) = \\frac{\\binom{N}{K}}{B(\\alpha, \\beta)} \\int_0^1 \\theta^{K+\\alpha-1} (1-\\theta)^{N-K+\\beta-1} d\\theta = \n",
    "\\frac{\\binom{N}{K} B(K+\\alpha, N-K + \\beta)}{B(\\alpha, \\beta)}\n",
    "\\end{equation}\n",
    "* Hence, the <span style=\"color:red\">posterior distribution</span> of $\\theta$ is again given by a Beta distribution\n",
    "\\begin{equation}\n",
    "\\color{red}{\\theta | K \\sim {\\rm Beta}(K + \\alpha, N-K + \\beta)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "alpha, beta = 2, 5\n",
    "N, K = 100, 10\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,4))\n",
    "xs = np.linspace(0, 1, 1000)\n",
    "ys = sts.beta.pdf(xs, alpha, beta)\n",
    "zs = sts.beta.pdf(xs, alpha+K, beta+N-K)\n",
    "\n",
    "ax.fill_between(xs, ys, label=\"prior $\\\\pi(\\\\theta)$\", alpha=0.5, color='tab:blue', linewidth=0)\n",
    "ax.fill_between(xs, zs, label=\"posterior $P(\\\\theta | K)$\", alpha=0.5, color='tab:red', linewidth=0)\n",
    "\n",
    "ax.plot(xs, ys, color='tab:blue', linewidth=2)\n",
    "ax.plot(xs, zs, color='tab:red', linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"$\\\\theta$\")\n",
    "ax.set_ylabel(\"$density$\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Inference \n",
    "**Example**\n",
    "\n",
    "Parameters: $\\alpha = 2$, $\\beta = 5$, $N = 100$, $K = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig ## show example figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov-Chain Monte Carlo\n",
    "* *Problem:* Normalizing constant $Z(D)$ (and other integrals) is almost always intractable.\n",
    "* *Solution:* We can generate a random sample from the posterior distribution **without knowing $Z(D)$**.\n",
    "* Using the random sample $(\\theta_i)_{i=1}^N$, we can approximate many statistics, e.g.\n",
    "$\\mathbb{E}[\\theta] \\approx \\frac1N \\sum_{i=1}^N \\theta_i$\n",
    "\n",
    "**Metropolis-Hastings algorithm**\n",
    "1. Start with a sample $\\theta_i$ \n",
    "2. Sample $\\theta_i'$ from proposal distribution $q(\\cdot | \\theta_i)$ (symmetric: $q(a|b) = q(b|a)$)\n",
    "3. **Accept** the proposed sample $\\theta_i'$ with probability \n",
    "\\begin{equation}\n",
    "\\min \\left\\{1, \\frac{Q(\\theta_i' | D)}{Q(\\theta_i | D)}\\right\\}\n",
    "\\end{equation}\n",
    "4. Next sample is\n",
    "\\begin{equation}\n",
    "\\theta_{i+1} = \\left\\{ \\begin{array}{ll} \\theta_{i}' & \\mbox{if accept} \\\\ \\theta_{i} & \\mbox{otherwise}  \\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "**Theorem:** The sequence $\\theta_1, \\theta_2, \\dots$ forms a Markov chain with stationary distribution equal to $P(\\theta|D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def MHstep(theta, q_rng, q_pdf, Q):\n",
    "    \"\"\"\n",
    "    Take a single Metropolis-Hastings step.\n",
    "    Start with a sample theta, and generate the next sample\n",
    "    using the proposal q_rng (to generate random numbers)\n",
    "    and q_pdf (the density), and the unnormalized posterior \n",
    "    density Q. The function returs the next sample, and whether or\n",
    "    not the parameter was accepted.\n",
    "    \"\"\"\n",
    "    theta_prop = q_rng(theta)\n",
    "    MH_prob = Q(theta_prop) * q_pdf(theta, theta_prop) / (Q(theta) * q_pdf(theta_prop, theta))\n",
    "    if sts.uniform.rvs() < MH_prob:\n",
    "        return theta_prop, True\n",
    "    return theta, False\n",
    "\n",
    "\n",
    "def MH_example_figure(Sigma):\n",
    "    \"\"\"\n",
    "    Create a figure to demonstrate the MH algorithm.\n",
    "    The target density is a multivariate normal distribution\n",
    "    with covariance matrix Sigma and mean 0.\n",
    "    \"\"\"\n",
    "    sigma = 0.7\n",
    "    \n",
    "    ## define proposal and posterior\n",
    "    def q_rng(theta):\n",
    "        return theta + sts.norm.rvs(loc=0, scale=sigma, size=2)\n",
    "\n",
    "    def q_pdf(theta_prop, theta):\n",
    "        return np.prod(sts.norm.pdf(theta_prop, loc=theta, scale=sigma))\n",
    "\n",
    "    def Q(theta):\n",
    "        return sts.multivariate_normal.pdf(theta, cov=Sigma)\n",
    "\n",
    "    theta = np.zeros(2)\n",
    "    N = 1000\n",
    "    thetas = [theta]\n",
    "    ar = 0 ## used to compute the fraction of accepted samples\n",
    "    \n",
    "    ## repeatedly take a Metropolis-Hastings step.\n",
    "    for i in range(N):\n",
    "        theta, acc = MHstep(theta, q_rng, q_pdf, Q)\n",
    "        thetas.append(theta)\n",
    "        if acc:\n",
    "            ar += 1\n",
    "    \n",
    "    ## make a figure of the results\n",
    "    fig = plt.figure(figsize=(7,3.5))\n",
    "    gs = GridSpec(2,2)\n",
    "    ax1 = fig.add_subplot(gs[1,0])\n",
    "    ax2 = fig.add_subplot(gs[0,0], sharex=ax1)\n",
    "    bx = fig.add_subplot(gs[:,1])\n",
    "\n",
    "    ax1.plot([x[0] for x in thetas], color='k')\n",
    "    ax2.plot([x[1] for x in thetas], color='k')\n",
    "    \n",
    "    ax1.set_ylabel(\"$\\\\theta^1$\")\n",
    "    ax2.set_ylabel(\"$\\\\theta^2$\")\n",
    "    ax1.set_xlabel(\"iteration\")\n",
    "\n",
    "    bx.plot([x[0] for x in thetas], [x[1] for x in thetas],\n",
    "            color='tab:red', linewidth=0.5, zorder=2)\n",
    "    bx.scatter([x[0] for x in thetas], [x[1] for x in thetas],\n",
    "               color='k', s=1,zorder=3)\n",
    "\n",
    "    plots.plot_cov_ellipse(bx, Sigma, np.zeros(2), nstd=2, alpha=0.3, zorder=1)\n",
    "    plots.plot_cov_ellipse(bx, Sigma, np.zeros(2), nstd=1, alpha=0.5, zorder=1)\n",
    "\n",
    "    w = 2.5\n",
    "    bx.set_xlim(-w,w)\n",
    "    bx.set_ylim(-w,w)\n",
    "    \n",
    "    bx.set_xlabel(\"$\\\\theta^1$\")\n",
    "    bx.set_ylabel(\"$\\\\theta^2$\")\n",
    "    \n",
    "    ## return the figure, the samples and the fraction of accepted samples\n",
    "    return fig, thetas, ar/N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov-Chain Monte Carlo\n",
    "**Metropolis Hastings Example**\n",
    "\n",
    "* Multivariate-Normal posterior density $\\theta | D \\sim \\mathcal{N}_2(0, \\Sigma)$\n",
    "* Normal proposal $\\theta_i' \\sim \\mathcal{N}_2(\\theta_i, \\sigma^2 I_2)$\n",
    "* chain length $N = 1000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Uncorrelated parameters\n",
    "\n",
    "$$\\Sigma = \\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = np.array([[1, 0], [0, 1]])\n",
    "fig, thetas, ar = MH_example_figure(Sigma)\n",
    "print(\"acceptance ratio:\", ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlated parameters\n",
    "$$\\Sigma = \\left(\\begin{array}{cc} 1 & 0.9 \\\\ 0.9 & 1 \\end{array} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = np.array([[1, 0.9], [0.9, 1]])\n",
    "fig, thetas, ar = MH_example_figure(Sigma)\n",
    "print(\"acceptance ratio:\", ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-correlated traces $\\implies$ small *effective* sample size $\\implies$ large Monte-Carlo error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonian Monte-Carlo\n",
    "\n",
    "* Essentially Metropolis-Mastings with a very fancy proposal\n",
    "* Extend parameters $\\theta \\in \\mathbb{R}^n$ with auxiliary parameter vector $p \\in \\mathbb{R}^n$ (called the \"conjugate momentum\")\n",
    "* Posterior density of $p$ is mulitvariate normal $p \\sim \\mathcal{N}_n(0, M)$. \n",
    "* Negative log-posterior joint density of $(\\theta, p)$ is called the \"Hamiltonian\"\n",
    "\\begin{equation}\n",
    " \\mathcal{H}(\\theta, p) = \\color{red}{-\\log(Q(\\theta | D))} + \\color{blue}{\\frac{1}{2} p^T M^{-1} p} + {\\rm constant}\n",
    "\\end{equation}\n",
    "* Interpretation: <span style=\"color:red\">potential energy</span> + <span style=\"color:blue\">kinetic energy</span>. Derive Hamilton's equations\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    " \\frac{d \\theta}{d t} &= \\frac{\\partial \\mathcal{H}}{\\partial p} \\\\\n",
    " \\frac{d p}{d t} &= -\\frac{\\partial \\mathcal{H}}{\\partial \\theta}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonian Monte-Carlo\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Start with state $(\\theta_i, p_i)$, sample $p_0 \\sim \\mathcal{N}_n(0, M)$ directly from marginal posterior\n",
    "2. Solve Hamiltonian initial value problem with $\\theta(0) = \\theta_i$ and $p(0) = p_0$ on the time interval $[0,T]$\n",
    "3. Set proposal $\\theta_i' = \\theta(T)$ and $p_i' = -p(T)$ (minus sign makes it symmetric)\n",
    "4. **Accept** proposed state with probability\n",
    "\\begin{equation}\n",
    " \\min\\left\\{1, \\exp\\left(-\\mathcal{H}(\\theta_i', p_i') + \\mathcal{H}(\\theta_i, p_0)\\right) \\right\\}\n",
    "\\end{equation}\n",
    "5. Next state is\n",
    "\\begin{equation}\n",
    "(\\theta_{i+1}, p_{i+1}) = \\left\\{ \\begin{array}{ll} (\\theta_{i}', p_i') & \\mbox{if accept} \\\\ (\\theta_{i}, p_i) & \\mbox{otherwise}  \\end{array} \\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonian Monte-Carlo\n",
    "\n",
    "**Why does this work?**\n",
    "\n",
    "* The $(\\theta_i, p_0) \\mapsto (\\theta(T), p(T))$ is non-linear, but **preserves volume**, so we don't have to add a **\"Jacobian correction\"**.\n",
    "* Because of **conservation of energy** ($\\frac{d}{dt}\\mathcal{H} = 0$), we should accept *all* proposed states.\n",
    "* To get the posterior distribution of $\\theta$, we just marginalize out $p$\n",
    "\n",
    "See e.g. [Neal, *MCMC using Hamiltonian Dynamics*](https://arxiv.org/pdf/1206.1901.pdf) for many more details\n",
    "\n",
    "**How does this actually work?**\n",
    "\n",
    "* We have to know the gradient of $\\mathcal{H}$, and hence of $\\log(Q)$. Solution: automatic differentiation\n",
    "* We don't want to integrate a system of ODEs with high accuracy for every Markov transition. Solution: leapfrog integration algorithm with step size $\\epsilon$. Introduces numerical error, $\\implies$ <strike>conservation of energy</strike>, and therefore the requirement of the MH step.\n",
    "* Stan automatically sets algorithmic parameters (such as $M$, $\\epsilon$) during warmup phase of the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonan Monte-Carlo\n",
    "**why does this work so well?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def can_sys(t, y, Sigma):\n",
    "    x, p = y[:2], y[2:]\n",
    "    dx = p\n",
    "    dp = -np.linalg.solve(Sigma, x)\n",
    "    return np.concatenate([dx, dp])\n",
    "\n",
    "x0 = np.array([1,1])\n",
    "Sigma1 = np.array([[1.0,0.0],[0.0,1.0]])\n",
    "Sigma2 = np.array([[1.0,0.9],[0.9,1.0]])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "for ax, Sigma in zip(axs, [Sigma1, Sigma2]):\n",
    "    ax.scatter(*x0, color='tab:green', zorder=3, label=\"$\\\\theta(0)$\")\n",
    "    for i in range(10):\n",
    "        p0 = sts.norm.rvs(0, 1, size=2)\n",
    "        y0 = np.concatenate([x0, p0])\n",
    "        t_span = (0, 2.5)\n",
    "        t_eval = np.linspace(*t_span, 1000)\n",
    "    \n",
    "        sol = solve_ivp(lambda t,y: can_sys(t,y,Sigma), t_span, y0, t_eval=t_eval)\n",
    "\n",
    "        ax.plot(sol.y[0], sol.y[1], color='k', zorder=2)\n",
    "        lab = \"$\\\\theta(T)$\" if i == 0 else None\n",
    "        ax.scatter(sol.y[0,-1], sol.y[1,-1], color='r', zorder=3, label=lab)\n",
    "    plots.plot_cov_ellipse(ax, Sigma, np.zeros(2), nstd=2, alpha=0.3, zorder=1)\n",
    "    plots.plot_cov_ellipse(ax, Sigma, np.zeros(2), nstd=1, alpha=0.5, zorder=1)\n",
    "\n",
    "w = 2.5\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-w,w)\n",
    "    ax.set_ylim(-w,w)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig ## show HMC figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hamiltonan Monte-Carlo\n",
    "**Distributions with variable correlation**\n",
    "\n",
    "\\begin{equation} \n",
    "\\theta = (x,y) = (r \\cos(\\phi), r\\sin(\\phi)),\\quad (r, \\phi) \\sim \\mathcal{N}_2((1,0), \\Sigma)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sm = cmdstanpy.CmdStanModel(stan_file=\"../stan-models/circular_density.stan\")\n",
    "\n",
    "r = 1\n",
    "mu = np.array([r,0])\n",
    "\n",
    "wsq = 0.01\n",
    "hsq = 0.8\n",
    "\n",
    "h = np.sqrt(hsq)\n",
    "w = np.sqrt(wsq)\n",
    "\n",
    "Sigma = np.array([[wsq, 0], [0, hsq]])\n",
    "\n",
    "data_dict = {\n",
    "    \"mu\" : mu,\n",
    "    \"sigma\" : Sigma\n",
    "}\n",
    "\n",
    "sam = sm.sample(chains=1, data=data_dict, iter_sampling=5000, thin=5)\n",
    "\n",
    "def moondist(z, Sigma, mu):\n",
    "    x, y = z\n",
    "    r = np.sqrt(np.dot(z,z))\n",
    "    phi = np.arctan2(y, x)\n",
    "    u = np.array([r,phi])\n",
    "    return sts.multivariate_normal.pdf(u, mu, Sigma) / r    \n",
    "\n",
    "def gradmoondist(z, Sigma, mu):\n",
    "    x, y = z\n",
    "    r = np.sqrt(np.dot(z, z))\n",
    "    phi = np.arctan2(y, x)\n",
    "    u = np.array([r, phi])\n",
    "    J = np.array([[x/r, -y/r**2], [y/r, x/r**2]])\n",
    "    c = np.array([x/r**2, y/r**2])        \n",
    "    return - np.dot(J, np.linalg.solve(Sigma, u-mu)) - c\n",
    "\n",
    "def can_sys(t, y, Sigma, mu):\n",
    "    x, p = y[:2], y[2:]\n",
    "    dx = p\n",
    "    dp = gradmoondist(x, Sigma, mu)\n",
    "    return np.concatenate([dx, dp])\n",
    "\n",
    "x0 = np.array([1, 0])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "axs[0].scatter(*x0, color='tab:green', zorder=4, label=\"$\\\\theta(0)$\")\n",
    "for i in range(10):\n",
    "    p0 = sts.norm.rvs(0, 1, size=2)\n",
    "    y0 = np.concatenate([x0, p0])\n",
    "    t_span = (0, 2)\n",
    "    t_eval = np.linspace(*t_span, 1000)\n",
    "    \n",
    "    sol = solve_ivp(lambda t,y: can_sys(t,y, Sigma, mu), \n",
    "                    t_span, y0, t_eval=t_eval)\n",
    "\n",
    "    axs[0].plot(sol.y[0], sol.y[1], color='k', zorder=2)\n",
    "    lab = \"$\\\\theta(T)$\" if i == 0 else None\n",
    "    axs[0].scatter(sol.y[0,-1], sol.y[1,-1], color='r', zorder=3, label=lab)\n",
    "\n",
    "xs = np.linspace(-2, 2, 100)\n",
    "ys = np.linspace(-2, 2, 100)\n",
    "Ls = [[moondist(np.array([x,y]), Sigma, mu) for x in xs] for y in ys]  \n",
    "lLs = np.log(Ls)\n",
    "    \n",
    "m = np.max(lLs)\n",
    "axs[0].contourf(xs, ys, lLs, [m-2,m], colors=['tab:blue'], alpha=0.3)\n",
    "axs[0].contourf(xs, ys, lLs, [m-1,m], colors=['tab:blue'], alpha=0.3)\n",
    "\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "## plot Stan sample\n",
    "\n",
    "xs = sam.stan_variable(\"x\")\n",
    "\n",
    "axs[1].scatter(xs[:,0], xs[:,1], s=2, color='k', zorder=3)\n",
    "axs[1].plot(xs[:,0], xs[:,1], zorder=2, color='r', linewidth=0.5)\n",
    "\n",
    "for ax in axs:\n",
    "    k = 1.2\n",
    "    ax.set_xlim(-k, k)\n",
    "    ax.set_ylim(-k, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig ## show moon-shaped distribution example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTS: **no-U-turn sampler**. Avoid \"going around in circles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
